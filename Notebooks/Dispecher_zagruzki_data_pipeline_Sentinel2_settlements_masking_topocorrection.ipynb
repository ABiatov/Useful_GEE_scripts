{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVQ0r_SOGPZC"
      },
      "source": [
        "**to prevent disconnection of virtual machine paste this to console (Ctrl+Shift+I) and hit enter :**\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "}\n",
        "var clicker=setInterval(ClickConnect,60000)<p>\n",
        "**to stop:**clearInterval(clicker);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c757RUgYRkVg"
      },
      "outputs": [],
      "source": [
        "#@title Initialize environment { vertical-output: true, display-mode: \"form\" }\n",
        "import ee\n",
        "from google.colab import drive\n",
        "ee.Authenticate()\n",
        "ee.Initialize()\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "my_drive = GoogleDrive(gauth)\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "import os\n",
        "from google.colab import files\n",
        "from google.colab import output\n",
        "import time\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from scipy.interpolate import interp1d\n",
        "import glob\n",
        "output.clear()\n",
        "!pip install rasterio\n",
        "import rasterio\n",
        "from rasterio.windows import Window\n",
        "from rasterio import merge\n",
        "from rasterio.fill import fillnodata\n",
        "from rasterio import features\n",
        "from rasterio.windows import Window\n",
        "from rasterio.transform import Affine\n",
        "output.clear()\n",
        "!pip install fiona\n",
        "output.clear()\n",
        "import fiona\n",
        "import pickle\n",
        "from distutils.dir_util import copy_tree\n",
        "if not os.path.isdir('keras_unet_collection'):\n",
        "  copy_tree('gdrive/MyDrive/Arabian/keras_unet_collection', 'keras_unet_collection')\n",
        "output.clear()\n",
        "from keras_unet_collection.models import transunet_2d\n",
        "from keras_unet_collection.losses import *\n",
        "from keras_unet_collection.transformer_layers import patch_extract, patch_embedding\n",
        "from keras_unet_collection.activations import GELU, Snake\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import save_model\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.utils import custom_object_scope\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import gc\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyZbyEnXBW5X",
        "outputId": "0bd295f0-ee25-44eb-ff62-34bcaf64765c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60 polygons\n"
          ]
        }
      ],
      "source": [
        "#@title Prepare dataset on EE side+load functions { vertical-output: true, display-mode: \"form\" }\n",
        "# generate AOI from \".gpkg\"\n",
        "bands=28\n",
        "size=512\n",
        "halfsize=size//2\n",
        "overlap=halfsize//2\n",
        "parent='gdrive/MyDrive/GEE_Beholder_Tiles/'\n",
        "shapefile=parent+'val_grid1_4326.gpkg'\n",
        "shapefile3857=parent+'val_grid1_3857.gpkg'\n",
        "move_path=parent+'moved/'\n",
        "predicted_path=parent+'result/'\n",
        "std_path=parent+'std/'\n",
        "\n",
        "polygon_features = []\n",
        "with fiona.open(shapefile) as layer:\n",
        "    for feature in layer:\n",
        "        polygon_features.append(ee.Feature(feature))\n",
        "AOI = ee.FeatureCollection(polygon_features)#.union().geometry().bounds()\n",
        "print(len(polygon_features),'polygons')\n",
        "#obtain masked image collection on basis of AOI and parameters below\n",
        "STARTYEAR = 2019\n",
        "ENDYEAR = 2020\n",
        "STARTDAY=70\n",
        "ENDDAY=310\n",
        "INTERVAL = ENDDAY - STARTDAY\n",
        "scale = 30\n",
        "my_bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12',\n",
        "                'NDVI', 'ARVI', 'GNDVI', 'CIG', 'FE_2', 'FE_3', 'FO', 'FS', 'FIA', 'CCA',\n",
        "                'NDSI', 'FI', 'FI_1', 'FI_2',\n",
        "                'NDSnowI', 'WM', 'NBR', 'NBR_2' ];\n",
        "CLOUD_FILTER = 70\n",
        "CLD_PRB_THRESH = 10\n",
        "NIR_DRK_THRESH = 0.15\n",
        "CLD_PRJ_DIST = 2\n",
        "BUFFER = 80\n",
        "\n",
        "NODATA_PIXEL_PERCENTAGE = 80\n",
        "\n",
        "#RELIEF\n",
        "dem = ee.Image(\"USGS/SRTMGL1_003\");\n",
        "slope = ee.Terrain.slope(dem).clip(AOI)\n",
        "aspect = ee.Terrain.aspect(dem).clip(AOI)\n",
        "aspect_modifed = aspect.subtract(180).abs()\n",
        "\n",
        "WorldCover_ds = ee.ImageCollection(\"ESA/WorldCover/v100\").first();\n",
        "settlements_mask = ee.FeatureCollection('projects/gee-antonbiatov/assets/beholder_arabian_peninsula/settlements_mask')\n",
        "\n",
        "def to_int16(image):\n",
        "  return image.int16()\n",
        "def addNDVI(image):# add NDVI band   (NIR - RED) / (NIR + RED)\n",
        "  return image.addBands(image.expression('int16(((b(\"B8\") - b(\"B4\")) / (b(\"B8\") + b(\"B4\"))+1)*1000)').rename('NDVI'))\n",
        "def addARVI(image):# add ARVI band    (NIR – (2 * RED) + BLUE) / (NIR + (2 * RED) + BLUE)\n",
        "  return image.addBands(image.expression('int16( ( (b(\"B8\") - (2 * b(\"B4\")) + b(\"B2\")) / ( b(\"B8\") + (2 * b(\"B4\")) + b(\"B2\") ) +1 ) * 1000)').rename('ARVI'))\n",
        "def addGNDVI(image): # add GNDVI band    (NIR - GREEN) / (NIR + GREEN)\n",
        "  return image.addBands(image.expression('int16(((b(\"B8\") - b(\"B3\")) / (b(\"B8\") + b(\"B3\"))+1)*1000)').rename('GNDVI'))\n",
        "def addCIG(image):# add CIG band (Chlorophyll Index Green)   NIR / GREEN\n",
        "  return image.addBands(image.expression('int16((b(\"B8\") / b(\"B3\"))*1000)').rename('CIG'))\n",
        "def addFE_2(image):# add FE_2 band (Ferric Iron, Fe2+)   SWIR2 / NIR + GREEN / RED\n",
        "  return image.addBands(image.expression('int16(((b(\"B12\") / b(\"B8\")) + (b(\"B3\") / b(\"B4\")))*1000)').rename('FE_2'))\n",
        "def addFE_3(image): # add FE_3 band (Ferric Iron, Fe3+)   GREEN / RED\n",
        "  return image.addBands(image.expression('int16((b(\"B3\") / b(\"B4\"))*1000)').rename('FE_3'))\n",
        "def addFO(image): # add FO band (Ferric Oxides)   SWIR1 / NIR\n",
        "  return image.addBands(image.expression('int16((b(\"B11\") / b(\"B8\"))*1000)').rename('FO'))\n",
        "def addFS(image):# add FS band (Ferrous Silicates (Ferric Iron Alteration) )   SWIR2 / SWIR1\n",
        "  return image.addBands(image.expression('int16((b(\"B12\") / b(\"B11\"))*1000)').rename('FS'))\n",
        "def addFIA(image):# add FIA band ( Ferrous Iron Alteration )   SWIR1 / RedEdge\n",
        "  return image.addBands(image.expression('int16((b(\"B11\") / b(\"B5\"))*1000)').rename('FIA'))\n",
        "def addCCA(image):# add CCA band ( Clay Carbonate Alteration; Clay-sulfate-mica-marble index )   SWIR1 / SWIR2\n",
        "  return image.addBands(image.expression('int16((b(\"B11\") / b(\"B12\"))*1000)').rename('CCA'))\n",
        "def addNDSI(image): # add NDSI band ( Normalized Different Senescence Index )   (SWIR1 - RED) / (SWIR1 + RED)\n",
        "  return image.addBands(image.expression('int16(((b(\"B11\") - b(\"B4\")) / (b(\"B11\") + b(\"B4\"))+1)*1000)').rename('NDSI'))\n",
        "def addFI(image):# add FI band ( Ferrous iron – coarse-grained ferric iron – fire ash index )   (GREEN  + SWIR1 ) / (RED + NIR)\n",
        "  return image.addBands(image.expression('int16(((b(\"B3\") + b(\"B11\")) / (b(\"B4\") + b(\"B8\"))+1)*1000)').rename('FI'))\n",
        "def addFI_1(image):# add FI_1 band ( Ferric iron 1 “redness” index )   RED / BLUE\n",
        "  return image.addBands(image.expression('int16((b(\"B4\") / b(\"B2\"))*1000)').rename('FI_1'))\n",
        "def addFI_2(image): # add FI_2 band ( Ferric iron index 2 )   (RED / BLUE) * ((RED + SWIR1 ) / NIR)\n",
        "  return image.addBands(image.expression('int16((b(\"B4\") + b(\"B2\")) * ((b(\"B4\") + b(\"B11\")) / b(\"B8\")))').rename('FI_2'))\n",
        "def addNDSnowI(image): # add NDSnowI band ( Normalized Difference Snow Index )   (GREEN – SWIR1 ) / (GREEN + SWIR1 )\n",
        "  return image.addBands(image.expression('int16(((b(\"B3\") - b(\"B11\")) / (b(\"B3\") + b(\"B11\"))+1)*1000)').rename('NDSnowI'))\n",
        "def addWM(image):# add WM band ( Water Masking )   BLUE / SWIR1\n",
        "  return image.addBands(image.expression('int16((b(\"B2\") / b(\"B11\"))*1000)').rename('WM'))\n",
        "def addNBR(image):# add NBR band ( Normalized Burn Ratio )   (NIR - SWIR2) / (NIR + SWIR2)\n",
        "  return image.addBands(image.expression('int16(((b(\"B8\") - b(\"B12\")) / (b(\"B8\") + b(\"B12\"))+1)*1000)').rename('NBR'))\n",
        "def addNBR_2(image): # add NBR_2 band ( Normalized Burn Ratio 2 )   (SWIR1 – SWIR2) / (SWIR1 + SWIR2)\n",
        "  return image.addBands(image.expression('int16(((b(\"B11\") - b(\"B12\")) / (b(\"B11\") + b(\"B12\"))+1)*1000)').rename('NBR_2'))\n",
        "\n",
        "#  ////////////////////////////////////////////////////////////////////////////////\n",
        "# Function to calculate illumination condition (IC). Function by Patrick Burns and Matt Macander\n",
        "def illuminationCondition(img):\n",
        "\n",
        "    # Extract image metadata about solar position\n",
        "    SZ_rad = ee.Image.constant(ee.Number(img.get('MEAN_SOLAR_ZENITH_ANGLE'))).multiply(3.14159265359).divide(180).clip(img.geometry().buffer(10000))\n",
        "    SA_rad = ee.Image.constant(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')).multiply(3.14159265359).divide(180)).clip(img.geometry().buffer(10000))\n",
        "    # Creat terrain layers\n",
        "    slp = ee.Terrain.slope(dem).clip(img.geometry().buffer(10000))\n",
        "    slp_rad = ee.Terrain.slope(dem).multiply(3.14159265359).divide(180).clip(img.geometry().buffer(10000))\n",
        "    asp_rad = ee.Terrain.aspect(dem).multiply(3.14159265359).divide(180).clip(img.geometry().buffer(10000))\n",
        "\n",
        "    # Calculate the Illumination Condition (IC)\n",
        "    # slope part of the illumination condition\n",
        "    cosZ = SZ_rad.cos()\n",
        "    cosS = slp_rad.cos()\n",
        "    slope_illumination = cosS.expression(\"cosZ * cosS\",\n",
        "                                          {'cosZ': cosZ,\n",
        "                                           'cosS': cosS.select('slope')})\n",
        "    # aspect part of the illumination condition\n",
        "    sinZ = SZ_rad.sin()\n",
        "    sinS = slp_rad.sin()\n",
        "    cosAziDiff = (SA_rad.subtract(asp_rad)).cos()\n",
        "    aspect_illumination = sinZ.expression(\"sinZ * sinS * cosAziDiff\",\n",
        "                                           {'sinZ': sinZ,\n",
        "                                            'sinS': sinS,\n",
        "                                            'cosAziDiff': cosAziDiff})\n",
        "    # full illumination condition (IC)\n",
        "    ic = slope_illumination.add(aspect_illumination)\n",
        "\n",
        "    # Add IC to original image\n",
        "    img_plus_ic = ee.Image(img.addBands(ic.rename('IC')).addBands(cosZ.rename('cosZ')).addBands(cosS.rename('cosS')).addBands(slp.rename('slope')))\n",
        "    return img_plus_ic\n",
        "\n",
        "  # //////////////////////////////////////////////////////////////////////////////\n",
        "  # Function to apply the Sun-Canopy-Sensor + C (SCSc) correction method to each\n",
        "  # image. Function by Patrick Burns and Matt Macander\n",
        "def illuminationCorrection(img):\n",
        "    props = img.toDictionary()\n",
        "    st = img.get('system:time_start')\n",
        "\n",
        "    img_plus_ic = img;\n",
        "    mask1 = img_plus_ic.select('B8').gt(-0.1)\n",
        "    mask2 = (img_plus_ic.select('slope').gte(5)\n",
        "                            .And(img_plus_ic.select('IC').gte(0))\n",
        "                            .And(img_plus_ic.select('B8').gt(-0.1)))\n",
        "    img_plus_ic_mask2 = ee.Image(img_plus_ic.updateMask(mask2))\n",
        "\n",
        "    # Specify Bands to topographically correct\n",
        "    # bandList = ['B2','B3','B4','B8','B11','B12']\n",
        "    # bandList = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n",
        "    # bandList = [ 'B7'] # возникает ошибка :\n",
        "    # Error: Error in map(ID=20200825T180919_20200825T182151_T12SWJ):\n",
        "    # Number.divide: Parameter 'left' is required. (Error code: 3)\n",
        "    bandList = ['B2', 'B3', 'B4', 'B5', 'B6', 'B8', 'B8A', 'B11', 'B12']\n",
        "    compositeBands = img.bandNames()\n",
        "    nonCorrectBands = img.select(compositeBands.removeAll(bandList))\n",
        "\n",
        "    geom = ee.Geometry(img.get('system:footprint')).bounds().buffer(10000)\n",
        "\n",
        "    def apply_SCSccorr(band):\n",
        "        method = 'SCSc';\n",
        "        out = (img_plus_ic_mask2.select('IC', band)\n",
        "                            .reduceRegion(reducer= ee.Reducer.linearFit(), # Compute coefficients: a(slope), b(offset), c(b/a)\n",
        "              #                           geometry= ee.Geometry(img.geometry().buffer(-5000)), # trim off the outer edges of the image for linear relationship\n",
        "                                         geometry= ee.Geometry(img.geometry()),\n",
        "                                         scale= 30,\n",
        "                                         maxPixels= 10000000000\n",
        "                                         ))\n",
        "\n",
        "#        if (out == 'null'):\n",
        "#            return img_plus_ic_mask2.select(band)\n",
        "#        else:\n",
        "#            out_a = ee.Number(out.get('scale'))\n",
        "#            out_b = ee.Number(out.get('offset'))\n",
        "#            out_c = out_b.divide(out_a)\n",
        "\n",
        "        out_a = ee.Number(out.get('scale'))\n",
        "        out_b = ee.Number(out.get('offset'))\n",
        "        out_c = out_b.divide(out_a)\n",
        "      # Apply the SCSc correction\n",
        "        SCSc_output = img_plus_ic_mask2.expression(\n",
        "        \"((image * (cosB * cosZ + cvalue)) / (ic + cvalue))\", {\n",
        "        'image': img_plus_ic_mask2.select(band),\n",
        "        'ic': img_plus_ic_mask2.select('IC'),\n",
        "        'cosB': img_plus_ic_mask2.select('cosS'),\n",
        "        'cosZ': img_plus_ic_mask2.select('cosZ'),\n",
        "        'cvalue': out_c\n",
        "        })\n",
        "\n",
        "        return SCSc_output.int16()\n",
        "\n",
        "\n",
        "    img_SCSccorr = ee.Image(list(map(apply_SCSccorr,bandList))).addBands(img_plus_ic.select('IC'))\n",
        "\n",
        "    bandList_IC = ee.List([bandList, 'IC']).flatten()\n",
        "    img_SCSccorr = img_SCSccorr.unmask(img_plus_ic.select(bandList_IC)).select(bandList).int16()\n",
        "\n",
        "    return img_SCSccorr.addBands(nonCorrectBands).set(props).set('system:time_start',st)\n",
        "#    return img_SCSccorr.addBands(nonCorrectBands).setMulti(props).set('system:time_start',st)\n",
        "\n",
        "def topoCorrection(collection):\n",
        "    #call function to calculate illumination condition\n",
        "    collection = collection.map(illuminationCondition)\n",
        "    #call function to calculate illumination Correction\n",
        "    collection = collection.map(illuminationCorrection)\n",
        "    return(collection)\n",
        "\n",
        "def maskVegetation(image):\n",
        "  qa_1 = image.select('ARVI')\n",
        "  mask = qa_1.lt(1500)#.And(qa_1.gt(1000));\n",
        "  return image.updateMask(mask)\n",
        "def maskCloud2(image):\n",
        "  qa_1 = image.select('B2')\n",
        "  qa_2 = image.select('B3')\n",
        "  mask = qa_1.lt(2700).And(qa_2.lt(2700))\n",
        "  return image.updateMask(mask)\n",
        "\n",
        "def get_s2_sr_cld_col(little_aoi, STARTYEAR, ENDYEAR,STARTDAY,ENDDAY):\n",
        "    # Import and filter S2 SR.\n",
        "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
        "        .filterBounds(little_aoi)\n",
        "        .filter(ee.Filter.calendarRange(STARTYEAR, ENDYEAR, 'year'))\n",
        "        .filter(ee.Filter.dayOfYear(STARTDAY, ENDDAY))\n",
        "        .filter(ee.Filter.lte('NODATA_PIXEL_PERCENTAGE', NODATA_PIXEL_PERCENTAGE))\n",
        "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER))\n",
        "        .map(addNDVI).map(addARVI).map(addGNDVI).map(addCIG).map(addFE_2).map(addFE_3).map(addFO).map(addFS).map(addFIA).map(addCCA)\n",
        "        .map(addNDSI).map(addFI).map(addFI_1).map(addFI_2).map(addNDSnowI).map(addWM).map(addNBR).map(addNBR_2)\n",
        "        #.map(maskVegetation)\n",
        "        #.map(maskCloud2)\n",
        "        .map(to_int16))\n",
        "\n",
        "    # Import and filter s2cloudless.\n",
        "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
        "        .filterBounds(little_aoi)\n",
        "        .filter(ee.Filter.calendarRange(STARTYEAR, ENDYEAR, 'year'))\n",
        "        .filter(ee.Filter.dayOfYear(STARTDAY, ENDDAY)))\n",
        "    # Join the filtered s2cloudless collection to the SR collection by the 'system:index' property.\n",
        "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{'primary': s2_sr_col,'secondary': s2_cloudless_col,\n",
        "    'condition': ee.Filter.equals(**{'leftField': 'system:index','rightField': 'system:index'})}))\n",
        "\n",
        "def add_cloud_bands(img):\n",
        "    # Get s2cloudless image, subset the probability band.\n",
        "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
        "    # Condition s2cloudless by the probability threshold value.\n",
        "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
        "    # Add the cloud probability layer and cloud mask as image bands.\n",
        "    return img.addBands(ee.Image([cld_prb, is_cloud]))\n",
        "def add_shadow_bands(img):\n",
        "    # Identify water pixels from the SCL band.\n",
        "    not_water = img.select('SCL').neq(6)\n",
        "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
        "    SR_BAND_SCALE = 1e4\n",
        "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).rename('dark_pixels').multiply(not_water)\n",
        "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
        "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
        "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
        "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
        "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
        "        .select('distance')\n",
        "        .mask()\n",
        "        .rename('cloud_transform'))\n",
        "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
        "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
        "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
        "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))\n",
        "def add_cld_shdw_mask(img):\n",
        "    # Add cloud component bands.\n",
        "    img_cloud = add_cloud_bands(img)\n",
        "    # Add cloud shadow component bands.\n",
        "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
        "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
        "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
        "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
        "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
        "    is_cld_shdw = (is_cld_shdw.focalMin(2).focalMax(BUFFER*2/20)\n",
        "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
        "        .rename('cloudmask'))\n",
        "    # Add the final cloud-shadow mask to the image.\n",
        "    return img_cloud_shadow.addBands(is_cld_shdw)\n",
        "def apply_cld_shdw_mask(img):\n",
        "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
        "    not_cld_shdw = img.select('cloudmask').Not()\n",
        "    # Subset reflectance bands and update their masks, return the result.\n",
        "    return img.select(my_bands).updateMask(not_cld_shdw)\n",
        "\n",
        "# vector invert mask. Need to have field 'val' = 1\n",
        "def vect_invert_mask(vect):\n",
        "    image_reduced = vect.reduceToImage(['val'], 'mean')\n",
        "    pre_mask = image_reduced.unmask(0)\n",
        "    invert_pre_mask = pre_mask.Not()\n",
        "#    invert_mask = invert_pre_mask.selfMask()\n",
        "    return invert_pre_mask\n",
        "\n",
        "# Define a kernel for water and landscape bufferization.\n",
        "# kernel = ee.Kernel.circle(**{'radius': 1});\n",
        "kernel = ee.Kernel.circle(**{'radius': 2});\n",
        "\n",
        "# Mask generator\n",
        "# 10 Trees, 20 Shrubland, 30 Grassland, 40 Cropland, 50 Built-up, 60 Barren / sparse vegetation,\n",
        "# 70 Snow and ice, 80 Open water, 90 Herbaceous wetland, 95 Mangroves, 100 Moss and lichen\n",
        "\n",
        "landscape_mask = (WorldCover_ds.select('Map').eq(30)  # Grassland\n",
        "    .Or(WorldCover_ds.select('Map').eq(40))  # Cropland\n",
        "    .Or(WorldCover_ds.select('Map').eq(60))  # Barren / sparse vegetation\n",
        "    .Or(WorldCover_ds.select('Map').eq(100)) # Moss and lichen\n",
        "    .rename('mask'))\n",
        "\n",
        "landscape_mask_buffered = landscape_mask.focalMin(**{'kernel': kernel, 'iterations': 1});\n",
        "mask = landscape_mask_buffered.updateMask(vect_invert_mask(settlements_mask)).selfMask().rename('mask')\n",
        "#def create_mask_tile(vector):\n",
        "#    mask_tile = mask.clip(vector).int16()\n",
        "#    return mask_tile\n",
        "\n",
        "def export_tile(poly,id):\n",
        "  AOI=ee.Geometry(poly.geometry())\n",
        "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, STARTYEAR, ENDYEAR,STARTDAY,ENDDAY)\n",
        "  #'''\n",
        "  s2_sr_cld_col_corr=topoCorrection(s2_sr_cld_col)\n",
        "  s2_sr_msk_col = (s2_sr_cld_col_corr.map(add_cld_shdw_mask).map(apply_cld_shdw_mask))\n",
        "  '''\n",
        "  s2_sr_msk_col = (s2_sr_cld_col.map(add_cld_shdw_mask).map(apply_cld_shdw_mask))\n",
        "  '''\n",
        "  med=s2_sr_msk_col.median().clip(AOI)\n",
        "  med_msk = med.updateMask(mask)\n",
        "  final_dataset = med_msk.addBands(dem.rename('elevation')).addBands(slope.multiply(10).rename('slope')).addBands(aspect_modifed.multiply(10).rename('aspect')).clip(AOI).int16()\n",
        "  #merged_masked_1 = merged.updateMask(vect_invert_mask(settlements_roads))\n",
        "  #merged_masked = merged_masked_1.updateMask(vect_invert_mask(wells_mask_buffer))\n",
        "  #bandNameslist=final_dataset.bandNames().getInfo()\n",
        "  #bandNameslist=list(map(lambda bandname:bandname[:8],bandNameslist[::10]))\n",
        "  #id=poly.getInfo()[\"id\"]\n",
        "  print('exporting to gdrive/MyDrive/GEE_Beholder_Tiles/'+id+'.tif...')\n",
        "  #print(bandNameslist)\n",
        "  #print(len(bandNameslist),'dates,',len(np.unique(bandNameslist)),'unique')\n",
        "  np.save('gdrive/MyDrive/GEE_Beholder_Tiles/'+id+'.npy',s2_sr_cld_col)\n",
        "  task=ee.batch.Export.image.toDrive(image = final_dataset, region=AOI, folder=\"GEE_Beholder_Tiles\",\n",
        "    description = id, crs='EPSG:3857', scale = scale, maxPixels = 1e10,fileFormat = 'GeoTIFF')\n",
        "  task.start()\n",
        "  return task\n",
        "\n",
        "if not os.path.isdir(move_path):os.mkdir(move_path)\n",
        "if not os.path.isdir(predicted_path):os.mkdir(predicted_path)\n",
        "if not os.path.isdir(std_path):os.mkdir(std_path)\n",
        "\n",
        "custom_objects = {\"patch_extract\": patch_extract,\"patch_embedding\":patch_embedding,\"GELU\":GELU,\"focal_tversky\":focal_tversky}\n",
        "models=[]\n",
        "with custom_object_scope(custom_objects):\n",
        "  for f in glob.glob(parent+'models/*.hdf5'):\n",
        "    models.append(load_model(f))\n",
        "\n",
        "percentile=np.load(parent+'percentile.npy').astype(np.float32)\n",
        "\n",
        "def interpolate(file):\n",
        "  masked_bands=28\n",
        "  other_bands=3\n",
        "  src=rasterio.open(file)\n",
        "  profile = src.profile\n",
        "  profile.update({\"count\":masked_bands+other_bands+1})\n",
        "  dest= rasterio.open(file +'f', 'w', **profile)\n",
        "  arr=src.read(1)\n",
        "  mask=~(arr==0)\n",
        "  dest.write_band(1, fillnodata(arr, mask=mask, max_search_distance=10, smoothing_iterations=0)-1)\n",
        "  for band in range(2,masked_bands+1):\n",
        "      arr=src.read(band)\n",
        "      dest.write_band(band, fillnodata(arr, mask=mask, max_search_distance=10, smoothing_iterations=0)-1)\n",
        "  for band in range(masked_bands+1,masked_bands+1+other_bands):\n",
        "    dest.write_band(band, src.read(band))\n",
        "  dest.write_band(masked_bands+other_bands+1, mask)\n",
        "  src.close()\n",
        "  dest.close()\n",
        "  shutil.move(file +'f',file)\n",
        "\n",
        "def infer(tiles,tile_index,direct):\n",
        "        X,out_transform = rasterio.merge.merge(tiles)\n",
        "        x,y=np.abs(direct)\n",
        "        X=X[:31,halfsize*y:halfsize*(y+2),halfsize*(x):halfsize*(x+2)]\n",
        "\n",
        "        X[X<-1]=-1\n",
        "        X=X.astype(np.float32)\n",
        "        X=np.transpose(X,(1,2,0))/percentile\n",
        "        X[X>1.0]=1.0\n",
        "\n",
        "        Y=models[0](np.expand_dims(X,0)[:,:,:,:bands])[:,overlap:size-overlap,overlap:size-overlap,:]\n",
        "        for model in models[1:]:\n",
        "          Y=np.concatenate((Y,model(np.expand_dims(X,0)[:,:,:,:bands])[:,overlap:size-overlap,overlap:size-overlap,:]),axis=3)\n",
        "        std=np.std(Y,axis=3)\n",
        "        Y=np.mean(Y,axis=3)\n",
        "\n",
        "        out_meta = tiles[0].meta.copy()\n",
        "        out_transform = Affine.translation(*rasterio.transform.xy(out_transform,halfsize*y+overlap,halfsize*x+overlap,offset='ul'))\n",
        "        out_transform *= Affine.scale(30, -30)\n",
        "        out_meta.update({\"height\": Y.shape[1],\"width\": Y.shape[2],\"transform\": out_transform,\"count\":1,\"dtype\":np.float32})\n",
        "\n",
        "        with rasterio.open(predicted_path+str(tuple(tile_index*2+1+direct))[1:-1].replace(', ','_')+'.tif', \"w\", **out_meta) as dest1:\n",
        "            dest1.write(Y)\n",
        "        with rasterio.open(std_path+str(tuple(tile_index*2+1+direct))[1:-1].replace(', ','_')+'.tif', \"w\", **out_meta) as dest2:\n",
        "            dest2.write(std)\n",
        "        Index1[tuple(tile_index*2+1+direct)] = str(tuple(tile_index*2+1+direct))\n",
        "        np.save(parent+'Index1.npy',Index1)\n",
        "\n",
        "def get_dims_coord(bounds):\n",
        "    return np.array([bounds[2]-bounds[0], bounds[3]-bounds[1]])\n",
        "\n",
        "def register_tile(file):\n",
        "    print('registering',file,'...')\n",
        "    tile = rasterio.open(file)\n",
        "    tile_index = np.around((np.array(rasterio.transform.xy(tile.transform,0,0))-start_coord)/tile_dims_coord,0).astype(np.uint16)+[1,0]\n",
        "    direct = np.array([0,0])\n",
        "    infer([tile],tile_index,direct)\n",
        "\n",
        "    vflag=0\n",
        "    for vdirect in np.array([[0,-1],[0,1]]):\n",
        "        v = Index[tuple(tile_index+vdirect)]\n",
        "        if v != \"\":\n",
        "            vtile=rasterio.open(v)\n",
        "            del v\n",
        "            infer([tile,vtile],tile_index,vdirect)\n",
        "            for hdirect in np.array([[-1,0],[1,0]]):\n",
        "                h = Index[tuple(tile_index+hdirect)]\n",
        "                if h != \"\":\n",
        "                    htile=rasterio.open(h)\n",
        "                    del h\n",
        "                    if Index1[tuple(tile_index*2+1+hdirect)]==\"\":\n",
        "                        infer([tile,htile],tile_index,hdirect)\n",
        "                    cdirect = vdirect + hdirect\n",
        "                    c = Index[tuple(tile_index+cdirect)]\n",
        "                    if c != \"\":\n",
        "                        ctile=rasterio.open(c)\n",
        "                        del c\n",
        "                        infer([tile,vtile,htile,ctile],tile_index,cdirect)\n",
        "                        ctile.close()\n",
        "                    htile.close()\n",
        "            vtile.close()\n",
        "\n",
        "            vflag=1\n",
        "    if vflag==0:\n",
        "        for hdirect in np.array([[-1,0],[1,0]]):\n",
        "            h = Index[tuple(tile_index+hdirect)]\n",
        "            if h != \"\":\n",
        "                htile=rasterio.open(h)\n",
        "                del h\n",
        "                infer([tile,htile],tile_index,hdirect)\n",
        "                htile.close()\n",
        "    tile.close()\n",
        "\n",
        "    Index[tuple(tile_index)] = file\n",
        "    np.save(parent+'Index.npy',Index)\n",
        "    #print(np.flip(Index.transpose(),0))\n",
        "\n",
        "    directs0=np.array([[i,ii] for i in range(-1,2) for ii in range(-1,2)])\n",
        "    directs=directs0[np.any(directs0!=0,axis=1)]\n",
        "    for d in directs:\n",
        "        if Index[tuple(tile_index+d)] != \"\":\n",
        "            Counter[tuple(tile_index+d)] += 1\n",
        "            Counter[tuple(tile_index)] += 1\n",
        "            np.save(parent+'Counter.npy',Counter)\n",
        "    for d in directs0:\n",
        "        if Counter[tuple(tile_index+d)]==8:\n",
        "            file=Index[tuple(tile_index+d)]\n",
        "            shutil.move(file, move_path+file.split('/')[-1])\n",
        "\n",
        "if not os.path.isfile(parent+'coords.npy'):\n",
        "    with fiona.open(shapefile3857) as source:\n",
        "        tile_dims_coord = get_dims_coord(features.bounds(next(iter(source))['geometry']))\n",
        "        dims = np.around(get_dims_coord(source.bounds)/tile_dims_coord,0).astype(np.uint16)\n",
        "        Index = np.array([[\"\" for _ in range(dims[1]+2)] for _ in range(dims[0]+2)],dtype = object)\n",
        "        Counter = np.zeros(dims+2,dtype = np.uint8)\n",
        "        Index1 = np.array([[\"\" for _ in range(Index.shape[1]*2+1)] for _ in range(Index.shape[0]*2+1)],dtype=object)\n",
        "        start_coord = np.array(source.bounds)[:2]\n",
        "    np.save(parent+'coords.npy',np.array([tile_dims_coord,start_coord]))\n",
        "    np.save(parent+'Index.npy',Index)\n",
        "    np.save(parent+'Counter.npy',Counter)\n",
        "    np.save(parent+'Index1.npy',Index1)\n",
        "else:\n",
        "    tile_dims_coord,start_coord = np.load(parent+'coords.npy')[[0,1]]\n",
        "    Index = np.load(parent+'Index.npy',allow_pickle=True)\n",
        "    Index1 = np.load(parent+'Index1.npy',allow_pickle=True)\n",
        "    Counter = np.load(parent+'Counter.npy',allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeFsiSp2aDL6"
      },
      "outputs": [],
      "source": [
        "#@title Export+preprocessing of geodata { vertical-output: true, display-mode: \"form\" }\n",
        "first =  1#@param {type:\"integer\"}\n",
        "last =  60#@param {type:\"integer\"}\n",
        "#automatic batch export from Earth Engine and postprocessing\n",
        "#from pprint import pprint\n",
        "\n",
        "if os.path.isfile(parent+'tasks.pkl'):\n",
        "  tasks=pickle.load(open(parent+'tasks.pkl','rb'))\n",
        "  names=list(np.load(parent+'names.npy'))\n",
        "else:\n",
        "  tasks=[]\n",
        "  names=[]\n",
        "\n",
        "for id,poly in enumerate(polygon_features[first-1:last]):\n",
        "  name=parent+str(id+first)#poly.getInfo()[\"id\"]\n",
        "  if name+'.tif' not in Index:\n",
        "      if not os.path.isfile(name+'.npy'):\n",
        "        names.append([name+'.tif',80])\n",
        "        tasks.append(export_tile(poly,str(id+first)))\n",
        "        pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "        np.save(parent+'names.npy',names)\n",
        "        #time.sleep(20)\n",
        "      i=0\n",
        "      while i<len(names):\n",
        "        if tasks[i].status()['state']=='FAILED':\n",
        "          #time.sleep(10)\n",
        "          names[i][1] = int(names[i][1]) - 10\n",
        "          print(names[i][1])\n",
        "          NODATA_PIXEL_PERCENTAGE = int(names[i][1])\n",
        "          tile_i=int(names[i][0].split('/')[-1][:-4])-1\n",
        "          tasks[i] = export_tile(polygon_features[tile_i],str(tile_i+1))\n",
        "          pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "          NODATA_PIXEL_PERCENTAGE = 80\n",
        "        if tasks[i].status()['state']=='RUNNING':\n",
        "          if int(tasks[i].status()['attempt'])>2:\n",
        "            #time.sleep(10)\n",
        "            names[i][1] = int(names[i][1]) - 10\n",
        "            NODATA_PIXEL_PERCENTAGE = int(names[i][1])\n",
        "            tile_i=int(names[i][0].split('/')[-1][:-4])-1\n",
        "            tasks[i].cancel()\n",
        "            tasks[i] = export_tile(polygon_features[tile_i],str(tile_i+1))\n",
        "            pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "            NODATA_PIXEL_PERCENTAGE = 80\n",
        "        if tasks[i].status()['state']=='COMPLETED':\n",
        "          time.sleep(5)\n",
        "          while not os.path.isfile(names[i][0]):\n",
        "            time.sleep(5)\n",
        "          interpolate(names[i][0])\n",
        "          time.sleep(20)\n",
        "          register_tile(names[i][0])\n",
        "          names.pop(i)\n",
        "          tasks.pop(i)\n",
        "          pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "          np.save(parent+'names.npy',names)\n",
        "        else:\n",
        "          i+=1\n",
        "        if (len(names)>150) and (i==len(names)):i=0\n",
        "\n",
        "while len(names):\n",
        "  i=0\n",
        "  while i<len(names):\n",
        "    if tasks[i].status()['state']=='FAILED':\n",
        "      names[i][1] = int(names[i][1]) - 10\n",
        "      NODATA_PIXEL_PERCENTAGE = int(names[i][1])\n",
        "      tile_i=int(names[i][0].split('/')[-1][:-4])-1\n",
        "      tasks[i] = export_tile(polygon_features[tile_i],str(tile_i+1))\n",
        "      pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "      NODATA_PIXEL_PERCENTAGE = 80\n",
        "    if tasks[i].status()['state']=='RUNNING':\n",
        "      if int(tasks[i].status()['attempt'])>2:\n",
        "        names[i][1] = int(names[i][1]) - 10\n",
        "        NODATA_PIXEL_PERCENTAGE = int(names[i][1])\n",
        "        tile_i=int(names[i][0].split('/')[-1][:-4])-1\n",
        "        tasks[i].cancel()\n",
        "        tasks[i] = export_tile(polygon_features[tile_i],str(tile_i+1))\n",
        "        pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "        NODATA_PIXEL_PERCENTAGE = 80\n",
        "    if tasks[i].status()['state']=='COMPLETED':\n",
        "      time.sleep(15)\n",
        "      while not os.path.isfile(names[i][0]):\n",
        "        time.sleep(10)\n",
        "      interpolate(names[i][0])\n",
        "      time.sleep(20)\n",
        "      register_tile(names[i][0])\n",
        "      names.pop(i)\n",
        "      tasks.pop(i)\n",
        "      pickle.dump(tasks, open(parent+'tasks.pkl', 'wb'))\n",
        "      np.save(parent+'names.npy',names)\n",
        "    else:\n",
        "      i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BAXFVXah1Ak"
      },
      "outputs": [],
      "source": [
        "names[1][1]=90"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grOTFrFSMRvV",
        "outputId": "d1a4fa66-a22d-475a-9732-d1f4200c0c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yelSUxt1lzhs",
        "outputId": "24e4530d-eb23-49bb-b874-aca374bb80cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exporting to gdrive/MyDrive/GEE_Beholder_Tiles/1.tif...\n",
            "exporting to gdrive/MyDrive/GEE_Beholder_Tiles/2.tif...\n"
          ]
        }
      ],
      "source": [
        "tasks=[]\n",
        "NODATA_PIXEL_PERCENTAGE = 40\n",
        "tasks.append(export_tile(polygon_features[0],'1'))\n",
        "tasks.append(export_tile(polygon_features[1],'2'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTOpuez_l8Lt"
      },
      "outputs": [],
      "source": [
        "print(tasks[1].status()['state'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u32jtLF_mBkb"
      },
      "outputs": [],
      "source": [
        "pickle.dump(tasks, open('1.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5SJAY30mVCC"
      },
      "outputs": [],
      "source": [
        "a=pickle.load(open('1.pkl','rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EuV5Ce2nL2U"
      },
      "outputs": [],
      "source": [
        "print(a[0].status())\n",
        "print(a[1].status())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg34beQjoFv9"
      },
      "outputs": [],
      "source": [
        "a[0].cancel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYy7RXmgzGIx"
      },
      "outputs": [],
      "source": [
        "x=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSExWEpuzP2x"
      },
      "outputs": [],
      "source": [
        "x.append(['b',2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9HYh6Df0lcA"
      },
      "outputs": [],
      "source": [
        "x[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lDebWx8bL12"
      },
      "outputs": [],
      "source": [
        "for task in tasks:task.cancel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK8UbMmMDN9K"
      },
      "outputs": [],
      "source": [
        "#only export\n",
        "N=3\n",
        "for i in range(1,2):\n",
        "  export_tile(polygon_features[i-1],str(i))\n",
        "  time.sleep(10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}